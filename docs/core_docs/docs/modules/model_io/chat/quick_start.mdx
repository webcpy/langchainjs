---
sidebar_position: 0
---
# 快速开始

Chat models是语言模型的一种变体。
虽然chat models在内部使用语言模型，但它们使用的接口有些不同。
与使用“文本输入，文本输出”的API不同，它们使用的接口是以“聊天消息”作为输入和输出。

## 设置

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import IntegrationInstallTooltip from "@mdx\_components/integration\_install\_tooltip.mdx";

<Tabs groupId="preferredModel">
  <TabItem value="openai" label="OpenAI" default>

首先，我们需要安装LangChain OpenAI集成包：

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai
```

访问API需要API密钥，您可以通过创建帐户并访问[这里](https://platform.openai.com/account/api-keys)来获取。一旦我们有了密钥，我们希望将其设置为环境变量：

```bash
OPENAI_API_KEY="..."
```

如果您不愿意设置环境变量，您可以在初始化OpenAI聊天模型类时通过`openAIApiKey`命名参数直接传递密钥：

```typescript
import { ChatOpenAI } from "@langchain/openai";

const chatModel = new ChatOpenAI({
  openAIApiKey: "...",
});
```

否则，您可以在不传递任何参数的情况下初始化：

```typescript
import { ChatOpenAI } from "@langchain/openai";

const chatModel = new ChatOpenAI();
```

  </TabItem>
  <TabItem value="local" label="本地（使用Ollama）">

[Ollama](https://ollama.ai/) 允许您在本地运行开源大型语言模型，如Llama 2和Mistral。

首先，按照[这些说明](https://github.com/jmorganca/ollama)设置并运行本地Ollama实例：

- [下载](https://ollama.ai/download)
- 通过例如 `ollama pull mistral` 获取模型

然后，确保Ollama服务器正在运行。接下来，您需要安装LangChain社区包：

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community
```

然后您可以执行：

```typescript
import { ChatOllama } from "@langchain/community/chat_models/ollama";

const chatModel = new ChatOllama({
  baseUrl: "http://localhost:11434", // 默认值
  model: "mistral",
});
```

  </TabItem>
  <TabItem value="anthropic" label="Anthropic" default>

首先，我们需要安装LangChain Anthropic集成包：

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/anthropic
```

访问API需要API密钥，您可以通过在[这里](https://platform.openai.com/account/api-keys)创建一个帐户来获取。

一旦我们有了一个密钥，我们就希望将其设置为环境变量：

```bash
ANTHROPIC_API_KEY="..."
```

如果您不愿意设置环境变量，您可以在初始化`ChatAnthropic`类时通过`anthropicApiKey`命名参数直接传递密钥：

```typescript
import { ChatAnthropic } from "@langchain/anthropic";

const chatModel = new ChatAnthropic({
  anthropicApiKey: "...",
});
```

否则，您可以在不带任何参数的情况下初始化：

```typescript
import { ChatAnthropic } from "@langchain/anthropic";

const chatModel = new ChatAnthropic();
```

  </TabItem>
  <TabItem value="google" label="Google GenAI" default>

首先，我们需要安装LangChain Google GenAI集成包：

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/google-genai
```
访问API需要API密钥，您可以通过在[这里](https://ai.google.dev/tutorials/setup)创建一个帐户来获取。
一旦我们有了密钥，我们希望将其设置为环境变量：

```bash
GOOGLE_API_KEY="..."
```

如果您不愿意设置环境变量，您可以在初始化`ChatGoogleGenerativeAI`类时通过`apiKey`命名参数直接传递密钥：

```typescript
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";

const chatModel = new ChatGoogleGenerativeAI({
  apiKey: "...",
});
```

否则，您可以在不传递任何参数的情况下初始化：

```typescript
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";

const chatModel = new ChatGoogleGenerativeAI();
```

  </TabItem>
</Tabs>

## 消息


聊天模型接口是基于消息而不是原始文本的。
目前在LangChain中支持的消息类型有`AIMessage`、`HumanMessage`、`SystemMessage`、`FunctionMessage和ChatMessage `
-- ChatMessage接受一个任意的role参数。大多数情况下，您只需要处理`HumanMessage`、`AIMessage和SystemMessage`。

## LCEL

聊天模型实现了[Runnable 接口](/docs/expression_language/interface)，
这是[LangChain表达语言 (LCEL)](/docs/expression_language/)的基本构建块。这意味着它们支持invoke、stream、batch和streamLog调用。

聊天模型接受 `BaseMessage[]` 作为输入，或者可以转换为消息的对象，包括 `字符串`（转换为 `HumanMessage` ）和 `PromptValue`。

```typescript
import { HumanMessage, SystemMessage } from "@langchain/core/messages";

const messages = [
  new SystemMessage("You're a helpful assistant"),
  new HumanMessage("What is the purpose of model regularization?"),
];
```

```typescript
await chatModel.invoke(messages);
```

```
AIMessage { content: 'The purpose of model regularization is to prevent overfitting in machine learning models. Overfitting occurs when a model becomes too complex and starts to fit the noise in the training data, leading to poor generalization on unseen data. Regularization techniques introduce additional constraints or penalties to the model's objective function, discouraging it from becoming overly complex and promoting simpler and more generalizable models. Regularization helps to strike a balance between fitting the training data well and avoiding overfitting, leading to better performance on new, unseen data.' }
```

查看[Runnable 接口](/docs/expression_language/interface)提供了更多关于可用方法的详细信息。

## [LangSmith](https://docs.smith.langchain.com/)

LangSmith跟踪是指LangChain为`ChatModel`提供的内置跟踪功能。只需设置以下环境变量即可启用：


```bash
export LANGCHAIN_TRACING_V2="true"
export LANGCHAIN_API_KEY=<your-api-key>
```


并且任何`ChatModel`的调用（无论是否嵌套在链中）都将自动被追踪。追踪结果将包括输入、输出、延迟、token使用情况、调用参数、环境参数等。您可以在此处查看一个示例: https://smith.langchain.com/public/a54192ae-dd5c-4f7a-88d1-daa1eaba1af7/r

在LangSmith中，您可以为任何追踪结果提供反馈，编译带有注释的数据集用于评估，在Playground中调试性能等。

## [旧版] `generate`

### 批量调用，更丰富的输出

您可以进一步使用`generate`为多组消息生成完成结果。这将返回一个LLMResult，并带有一个额外的消息参数。


```typescript
const response3 = await chatModel.generate([
  [
    new SystemMessage(
      "You are a helpful assistant that translates English to French."
    ),
    new HumanMessage(
      "Translate this sentence from English to French. I love programming."
    ),
  ],
  [
    new SystemMessage(
      "You are a helpful assistant that translates English to French."
    ),
    new HumanMessage(
      "Translate this sentence from English to French. I love artificial intelligence."
    ),
  ],
]);
console.log(response3);
/*
  {
    generations: [
      [
        {
          text: "J'aime programmer.",
          message: AIMessage { text: "J'aime programmer." },
        }
      ],
      [
        {
          text: "J'aime l'intelligence artificielle.",
          message: AIMessage { text: "J'aime l'intelligence artificielle." }
        }
      ]
    ]
  }
*/
```

You can recover things like token usage from this LLMResult:

```typescript
console.log(response3.llmOutput);
/*
  {
    tokenUsage: { completionTokens: 20, promptTokens: 69, totalTokens: 89 }
  }
*/
```
