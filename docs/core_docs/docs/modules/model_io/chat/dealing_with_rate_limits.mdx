# 处理速率限制

某些提供商设置了速率限制。如果您超过了这个限制，将会收到一个错误。为了帮助您处理这个问题，LangChain 在实例化聊天模型时提供了一个 `maxConcurrency` 选项。这个选项允许您指定想要向提供商发送的最大并发请求数。如果您超过了这个数字，LangChain 将自动将您的请求排队，等到之前的请求完成后发送。

例如，如果您设置了 `maxConcurrency: 5`，那么 LangChain 一次只会向提供商发送 5 个请求。如果您发送了 10 个请求，前 5 个会立即发送，接下来的 5 个将会排队。一旦前 5 个请求中的一个完成了，队列中的下一个请求将会被发送。

要使用这个功能，您在实例化 LLM 时简单地传递 `maxConcurrency: <number>` 即可。例如：

```bash npm2yarn
npm install @langchain/openai
```

```typescript
import { ChatOpenAI } from "@langchain/openai";

const model = new ChatOpenAI({ maxConcurrency: 5 });
```

