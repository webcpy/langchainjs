---
sidebar_position: 1
---
# Streaming(流式传输)

一些聊天模型提供流式响应。这意味着你不必等待整个响应返回，而是可以在可用时立即开始处理它。
这在你想要在生成过程中向用户显示响应，或者在生成过程中处理响应时非常有用。

## 使用 `.stream()`

import CodeBlock from "@theme/CodeBlock";

使用流式传输的最简单方法是使用 `.stream()` 方法。这将返回一个可读流，您也可以对其进行迭代：

import StreamMethodExample from "@examples/models/chat/chat_streaming_stream_method.ts";

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai
```

<CodeBlock language="typescript">{StreamMethodExample}</CodeBlock>

对于不支持流式传输的模型，整个响应将作为一个数据块返回。

为了方便起见，您还可以将聊天模型传递给 [StringOutputParser](/docs/modules/model_io/output_parsers/types/string)，以从每个块中提取出纯文本字符串值。

import StringExample from "@examples/prompts/string_output_parser.ts";

<CodeBlock language="typescript">{StringExample}</CodeBlock>


您还可以使用 [HttpResponseOutputParser](/docs/modules/model_io/output_parsers/types/http_response) 类似地直接流式传输字节（例如，用于在 HTTP 响应中返回流）。

import HttpExample from "@examples/prompts/http_response_output_parser.ts";

<CodeBlock language="typescript">{HttpExample}</CodeBlock>

## 使用回调处理程序

您还可以像这样使用一个 [`CallbackHandler`](https://github.com/langchain-ai/langchainjs/blob/main/langchain/src/callbacks/base.ts)：

import StreamingExample from "@examples/models/chat/chat_streaming.ts";

<CodeBlock language="typescript">{StreamingExample}</CodeBlock>

