---
sidebar_position: 3
---
# 缓存

LangChain为聊天模型提供了一个可选的缓存层。下面的两个原因是很有用的：

- 如果您经常多次请求相同的完成结果，它可以通过减少您向LLM提供程序发出的API调用次数来节省您的费用。
- 通过减少您向LLM提供程序发出的API调用次数，可以加快应用程序的速度。

import CodeBlock from "@theme/CodeBlock";

```typescript
import { ChatOpenAI } from "@langchain/openai";

// 为了使缓存更加明显，让我们使用一个更慢的模型。
const model = new ChatOpenAI({
  modelName: "gpt-4",
  cache: true,
});
```

## 内存缓存

默认缓存存储在内存中。这意味着如果您重新启动应用程序，缓存将被清除。

```typescript
console.time();

// 第一次，它还没有在缓存中，所以应该需要更长时间
const res = await model.invoke("Tell me a joke!");
console.log(res);

console.timeEnd();

/*
  AIMessage {
    lc_serializable: true,
    lc_kwargs: {
      content: "Why don't scientists trust atoms?\n\nBecause they make up everything!",
      additional_kwargs: { function_call: undefined, tool_calls: undefined }
    },
    lc_namespace: [ 'langchain_core', 'messages' ],
    content: "Why don't scientists trust atoms?\n\nBecause they make up everything!",
    name: undefined,
    additional_kwargs: { function_call: undefined, tool_calls: undefined }
  }
  default: 2.224s
*/
```

```typescript
console.time();

// 第二次，它已经在缓存中，所以速度更快
const res2 = await model.invoke("Tell me a joke!");
console.log(res2);

console.timeEnd();
/*
  AIMessage {
    lc_serializable: true,
    lc_kwargs: {
      content: "Why don't scientists trust atoms?\n\nBecause they make up everything!",
      additional_kwargs: { function_call: undefined, tool_calls: undefined }
    },
    lc_namespace: [ 'langchain_core', 'messages' ],
    content: "Why don't scientists trust atoms?\n\nBecause they make up everything!",
    name: undefined,
    additional_kwargs: { function_call: undefined, tool_calls: undefined }
  }
  default: 181.98ms
*/
```

## 使用Momento进行缓存

LangChain还提供基于Momento的缓存。[Momento](https://gomomento.com)是一个分布式、无服务器缓存，无需任何设置或基础设施维护。要使用它，您需要安装`@gomomento/sdk`包：

```bash npm2yarn
npm install @gomomento/sdk
```

接下来，您需要注册并创建一个API密钥。完成后，在实例化LLM时传递一个`cache`选项，如下所示：

import MomentoCacheExample from "@examples/cache/chat\_models/momento.ts";

import IntegrationInstallTooltip from "@mdx\_components/integration\_install\_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai
```

<CodeBlock language="typescript">{MomentoCacheExample}</CodeBlock>

## 使用Redis进行缓存

LangChain还提供基于Redis的缓存。如果您希望在多个进程或服务器之间共享缓存，这将非常有用。要使用它，您需要安装`redis`包：

```bash npm2yarn
npm install ioredis
```

然后，在实例化LLM时，可以传递一个`cache`选项。例如：

```typescript
import RedisCacheExample from "@examples/cache/chat\_models/redis.ts";

<CodeBlock language="typescript">{RedisCacheExample}</CodeBlock>

## 使用Upstash Redis进行缓存

LangChain提供基于Upstash Redis的缓存。与基于Redis的缓存类似，如果您希望在多个进程或服务器之间共享缓存，这将非常有用。Upstash Redis客户端使用HTTP，并支持边缘环境。要使用它，您需要安装`@upstash/redis`包：

```bash npm2yarn
npm install @upstash/redis
```

您还需要一个[Upstash账户](https://docs.upstash.com/redis#create-account)和一个[Redis数据库](https://docs.upstash.com/redis#create-a-database)来连接。完成后，检索您的REST URL和REST token。

然后，在实例化LLM时，可以传递一个`cache`选项。例如：

```typescript
import UpstashRedisCacheExample from "@examples/cache/chat\_models/upstash\_redis.ts";

<CodeBlock language="typescript">{UpstashRedisCacheExample}</CodeBlock>

您还可以直接传递一个之前创建的[@upstash/redis](https://docs.upstash.com/redis/sdks/javascriptsdk/overview)客户端实例：

```typescript
import AdvancedUpstashRedisCacheExample from "@examples/cache/chat\_models/upstash\_redis\_advanced.ts";

<CodeBlock language="typescript">{AdvancedUpstashRedisCacheExample}</CodeBlock>

## 使用Cloudflare KV进行缓存

:::info
此集成仅在Cloudflare Workers中受支持。
:::

如果您将项目部署为Cloudflare Worker，您可以使用LangChain的Cloudflare KV驱动的LLM缓存。

有关如何在Cloudflare中设置KV的信息，请参阅[官方文档](https://developers.cloudflare.com/kv/)。

**注意：** 如果您使用TypeScript，如果尚未存在类型，则可能需要安装类型：

```bash npm2yarn
npm install -S @cloudflare/workers-types
```

```typescript
import CloudflareExample from "@examples/cache/chat\_models/cloudflare\_kv.ts";

<CodeBlock language="typescript">{CloudflareExample}</CodeBlock>

## 文件系统缓存

:::warning
不建议在生产环境中使用此缓存。它仅用于本地开发。
:::

LangChain提供了一个简单的文件系统缓存。
默认情况下，缓存存储在临时目录中，但如果需要，您可以指定一个自定义目录。

```typescript
const cache = await LocalFileCache.create();
```

