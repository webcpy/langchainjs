---
sidebar_position: 0
---
# 快速入门

大型语言模型（LLMs）是LangChain的核心组件。LangChain不提供自己的LLMs，
而是提供了一个标准接口，用于与许多不同的LLMs进行交互。


有许多LLM提供商（如OpenAI、Cohere、Hugging Face等），-`LLM`类的设计旨在为所有这些提供商提供一个标准接口。

在本教程中，我们将使用OpenAI LLM包装器，尽管突出显示的功能对所有LLM类型都是通用的。

## 设置

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import CodeBlock from "@theme/CodeBlock";

<Tabs>
  <TabItem value="openai" label="OpenAI" default>

首先，我们需要安装LangChain OpenAI集成包：

import IntegrationInstallTooltip from "@mdx\_components/integration\_install\_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai
```

访问API需要一个API密钥，您可以通过创建帐户并转到[此处](https://platform.openai.com/account/api-keys)来获取密钥。一旦我们有了密钥，我们将希望通过运行以下命令将其设置为环境变量：

```bash
export OPENAI_API_KEY="..."
```

如果您不想设置环境变量，您可以在初始化OpenAI Chat Model类时通过`openAIApiKey`命名参数直接传递密钥：

```typescript
import { OpenAI } from "@langchain/openai";

const llm = new OpenAI({
  openAIApiKey: "YOUR_KEY_HERE",
});
```

否则，您可以用空对象初始化：

```typescript
import { OpenAI } from "@langchain/openai";

const llm = new OpenAI({});
```

  </TabItem>
  <TabItem value="local" label="本地（使用Ollama）">

[Ollama](https://ollama.ai/)允许您在本地运行开源大型语言模型，例如Llama 2和Mistral。

首先，按照[这些说明](https://github.com/jmorganca/ollama)设置和运行本地Ollama实例：

- [下载](https://ollama.ai/download)
- 通过例如 `ollama pull mistral` 获取一个模型

然后，确保Ollama服务器正在运行。接下来，您需要安装LangChain社区包：

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community
```

然后您可以执行：

```typescript
import { Ollama } from "@langchain/community/llms/ollama";

const llm = new Ollama({
  baseUrl: "http://localhost:11434", // 默认值
  model: "mistral",
});
```

  </TabItem>
</Tabs>

## LCEL

LLMs实现了[Runnable接口](/docs/expression_language/interface)，这是[LangChain表达语言（LCEL）](/docs/expression_language/)的基本构建块。这意味着它们支持`invoke`、`stream`、`batch`和`streamLog`调用。

LLMs接受**字符串**作为输入，或者可以被强制转换为字符串提示的对象，包括`BaseMessage[]`和`PromptValue`。

```typescript
await llm.invoke(
  "关于失业和通货膨胀之间关系的一些理论是什么？"
);
```

```
'\n\n1. 菲利普斯曲线理论：这表明失业和通货膨胀之间存在反向关系，这意味着当失业率较低时，通货膨胀率会较高，而当失业率较高时，通货膨胀率会较低。\n\n2. 货币主义理论：这一理论表明失业和通货膨胀之间的关系较弱，货币供应的变化更重要。\n\n3. 资源利用理论：这表明当失业率较低时，企业能够提高工资和价格以利用对其产品和服务的需求增加。这导致通货膨胀率上升。'
```

请参阅[Runnable接口](/docs/expression_language/interface)，以获取有关可用方法的更多详细信息。

## \[旧版] `generate`: 批量调用，更丰富的输出

`generate`方法允许您使用字符串列表调用模型，获取比仅文本更完整的响应。这个完整的响应可以包括多个顶级响应和其他LLM提供商特定的信息。

```typescript
const llmResult = await llm.generate(
  ["给我讲个笑话", "给我念首诗"],
  ["给我讲个笑话", "给我念首诗"]
);

console.log(llmResult.generations.length);

// 30

console.log(llmResult.generations[0]);

/*
  [
    {
      text: "\n\n问：鱼撞墙时说了什么？\n答：坝！",
      generationInfo: { finishReason: "stop", logprobs: null }
    }
  ]
*/

console.log(llmResult.generations[1]);

/*
  [
    {
      text: "\n\n玫瑰是红色的，\n紫罗兰是蓝色的，\n糖是甜的，\n你也是。",
      generationInfo: { finishReason: "stop", logprobs: null }
    }
  ]
*/
```

您还可以访问返回的特定于提供商的信息。这些信息在提供商之间并未标准化。

```typescript
console.log(llmResult.llmOutput);

/*
  {
    tokenUsage: { completionTokens: 46, promptTokens: 8, totalTokens: 54 }
  }
*/
```

import AdvancedExample from "@examples/models/llm/llm\_advanced.ts";

这是一个带有额外参数的示例，它将`max_tokens`设置为`-1`以打开令牌标记大小计算：

<CodeBlock language="typescript">{AdvancedExample}</CodeBlock>

