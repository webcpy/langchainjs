# 处理速率限制

某些语言模型（LLM）提供商设置了速率限制。如果您超过了这个限制，将会收到一个错误。为了帮助您处理这个问题，LangChain 在实例化 LLM 时提供了一个 `maxConcurrency` 选项。这个选项允许您指定想要向 LLM 提供商发起的最大并发请求数。如果您超过了这个数字，LangChain 将会自动将您的请求排队，等到之前的请求完成后发送。

例如，如果您设置了 `maxConcurrency: 5`，那么 LangChain 一次只会向 LLM 提供商发送 5 个请求。如果您发送了 10 个请求，前 5 个将会立即发送，接下来的 5 个将会被排队。一旦前 5 个请求中的一个完成了，队列中的下一个请求将会被发送。

要使用这个特性，您在实例化 LLM 时简单传递 `maxConcurrency: <number>` 即可。例如：

```bash npm2yarn
npm install @langchain/openai
```

```typescript
import { OpenAI } from "@langchain/openai";

const model = new OpenAI({ maxConcurrency: 5 });
```

