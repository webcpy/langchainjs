---
sidebar_position: 2
---

# 缓存

LangChain为LLMs提供了一个可选的缓存层。这有两个好处：

- 如果您经常多次请求相同的完成结果，它可以通过减少您向LLM提供程序发出的API调用次数来节省您的费用。
- 通过减少您向LLM提供程序发出的API调用次数，可以加快应用程序的速度。

```bash npm2yarn
npm install @langchain/openai
```
import CodeBlock from "@theme/CodeBlock";

```typescript
import { OpenAI } from "@langchain/openai";

const model = new OpenAI({
  modelName: "gpt-3.5-turbo-instruct",
  cache: true,
});
```

## 内存缓存

默认缓存存储在内存中。这意味着如果您重新启动应用程序，缓存将被清除。

```typescript
console.time();

// 第一次，它尚未在缓存中，所以应该需要更长时间
const res = await model.invoke("Tell me a long joke");

console.log(res);

console.timeEnd();

/*
  一个人走进酒吧，看到柜台上有一个装满钱的罐子。好奇，他问侍者是怎么回事。

  侍者解释说：“我们给我们的顾客一个挑战。如果你能完成三项任务，你就赢得罐子里所有的钱。”

  这个人感到好奇，问任务是什么。

  侍者回答说：“首先，你必须喝下一整瓶龙舌兰酒而不皱眉。其次，后面有一只牙疼的斗牛犬。你必须把它拔出来。第三，楼上有一个从未达到高潮的老太太。你必须让她达到高潮。”

  这个人想了一会儿，然后自信地说：“我会做到。”

  他拿起龙舌兰酒一口气喝光，毫不畏惧。然后他走到后面，经过几分钟的挣扎，拿着斗牛犬的牙齿出来了。

  酒吧里爆发出欢呼声，侍者领着这个人上楼去老太太的房间。几分钟后，这个人带着笑容走出来，老太太在高兴地咯咯笑。

  侍者递给这个人罐子里的钱，问：“你怎么
  默认：4.187秒
*/
```

```typescript
console.time();

// 第二次，它在缓存中，所以速度更快
const res2 = await model.invoke("Tell me a joke");

console.log(res2);

console.timeEnd();

/*
  一个人走进酒吧，看到柜台上有一个装满钱的罐子。好奇，他问侍者是怎么回事。

  侍者解释说：“我们给我们的顾客一个挑战。如果你能完成三项任务，你就赢得罐子里所有的钱。”

  这个人感到好奇，问任务是什么。

  侍者回答说：“首先，你必须喝下一整瓶龙舌兰酒而不皱眉。其次，后面有一只牙疼的斗牛犬。你必须把它拔出来。第三，楼上有一个从未达到高潮的老太太。你必须让她达到高潮。”

  这个人想了一会儿，然后自信地说：“我会做到。”

  他拿起龙舌兰酒一口气喝光，毫不畏惧。然后他走到后面，经过几分钟的挣扎，拿着斗牛犬的牙齿出来了。

  酒吧里爆发出欢呼声，侍者领着这个人上楼去老太太的房间。几分钟后，这个人带着笑容走出来，老太太在高兴地咯咯笑。

  侍者递给这个人罐子里的钱，问：“你怎么
  默认：175.74毫秒
*/
```

## 使用Momento进行缓存

LangChain还提供基于Momento的缓存。[Momento](https://gomomento.com)是一个分布式、无服务器的缓存，无需任何设置或基础设施维护。鉴于Momento与Node.js、浏览器和边缘环境的兼容性，请确保您安装相关的软件包。

要安装**Node.js**：

```bash npm2yarn
npm install @gomomento/sdk
```

要安装**浏览器/边缘工作者**：

```bash npm2yarn
npm install @gomomento/sdk-web
```

接下来，您需要注册并创建一个API密钥。一旦完成，实例化LLM时传递一个`cache`选项，如下所示：

import MomentoCacheExample from "@examples/cache/momento.ts";

<CodeBlock language="typescript">{MomentoCacheExample}</CodeBlock>

## 使用Redis进行缓存

LangChain还提供基于Redis的缓存。如果您希望在多个进程或服务器之间共享缓存，这将非常有用。要使用它，您需要安装`redis`软件包：

```bash npm2yarn
npm install ioredis
```

然后，在实例化LLM时，您可以传递一个`cache`选项。例如：

```typescript
import { OpenAI } from "@langchain/openai";
import { RedisCache } from "@langchain/community/caches/ioredis";
import { Redis } from "ioredis";

// 有关连接选项，请参阅 https://github.com/redis/ioredis
const client = new Redis({});

const cache = new RedisCache(client);

const model = new OpenAI({ cache });

```

## 使用 Upstash Redis 进行缓存

LangChain 提供了基于 Upstash Redis 的缓存。与基于 Redis 的缓存一样，如果您希望在多个进程或服务器之间共享缓存，这种缓存非常有用。Upstash Redis 客户端使用 HTTP 并支持边缘环境。要使用它，您需要安装 `@upstash/redis` 包：

```bash npm2yarn
npm install @upstash/redis
```

您还需要一个 [Upstash 账户](https://docs.upstash.com/redis#create-account) 和一个 [Redis 数据库](https://docs.upstash.com/redis#create-a-database) 来连接。完成这些步骤后，获取您的 REST URL 和 REST token。

然后，在实例化 LLM 时，您可以传递一个 `cache` 选项。例如：


