---
sidebar_position: 1
---
# Streaming(流式传输)


一些大型语言模型（LLMs）支持Streaming(流式传输)响应。
这意味着你不必等待整个响应返回，就可以在它可用时立即开始处理。如果你想要在生成过程中向用户显示响应，或者想要在生成过程中处理响应，这将非常有用。

## 使用 `.stream()`

import CodeBlock from "@theme/CodeBlock";

最简单的Streaming(流式传输)方式是使用 `.stream()` 方法。这将返回一个可读流，你也可以对其进行迭代：

import StreamMethodExample from "@examples/models/llm/llm_streaming_stream_method.ts";

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai
```

<CodeBlock language="typescript">{StreamMethodExample}</CodeBlock>

对于不支持Streaming(流式传输)的模型，整个响应将作为单个块返回。

## 使用回调处理器

你也可以像这样使用一个 [`CallbackHandler`](https://github.com/langchain-ai/langchainjs/blob/main/langchain/src/callbacks/base.ts)：

import StreamingExample from "@examples/models/llm/llm_streaming.ts";

<CodeBlock language="typescript">{StreamingExample}</CodeBlock>

如果使用`generate`，我们仍然可以访问最终的`LLMResult`。然而，对于Streaming(流式传输)，目前不支持`token_usage`。






