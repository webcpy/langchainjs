---
sidebar_position: 4
---
# 自定义聊天模型

本笔记介绍如何创建一个自定义聊天模型包装器，以便您可以使用自己的聊天模型或与 LangChain 直接支持的包装器不同的包装器。

在扩展 [`SimpleChatModel` 类](https://api.js.langchain.com/classes/langchain_core_language_models_chat_models.SimpleChatModel.html) 后，聊天模型需要实现以下几个必需的内容：

- 一个 `_call` 方法，接受消息列表和调用选项（包括诸如 `stop` 序列之类的内容），并返回一个字符串。
- 一个 `_llmType` 方法，返回一个字符串。仅用于日志记录目的。

您还可以实现以下可选方法：

- 一个 `_streamResponseChunks` 方法，返回一个 `AsyncGenerator` 并产生 [`ChatGenerationChunks`](https://api.js.langchain.com/classes/langchain_core_outputs.ChatGenerationChunk.html)。这允许 LLM 支持流式输出。

让我们实现一个非常简单的自定义聊天模型，它只是回显输入的前 `n` 个字符。

```typescript
import {
  SimpleChatModel,
  type BaseChatModelParams,
} from "@langchain/core/language_models/chat_models";
import { CallbackManagerForLLMRun } from "@langchain/core/callbacks/manager";
import { AIMessageChunk, type BaseMessage } from "@langchain/core/messages";
import { ChatGenerationChunk } from "@langchain/core/outputs";

export interface CustomChatModelInput extends BaseChatModelParams {
  n: number;
}

export class CustomChatModel extends SimpleChatModel {
  n: number;

  constructor(fields: CustomChatModelInput) {
    super(fields);
    this.n = fields.n;
  }

  _llmType() {
    return "custom";
  }

  async _call(
    messages: BaseMessage[],
    options: this["ParsedCallOptions"],
    runManager?: CallbackManagerForLLMRun
  ): Promise<string> {
    if (!messages.length) {
      throw new Error("未提供消息。");
    }
    // 在调用内部运行时，传递 `runManager?.getChild()` 以启用跟踪
    // await subRunnable.invoke(params, runManager?.getChild());
    if (typeof messages[0].content !== "string") {
      throw new Error("不支持多模态消息。");
    }
    return messages[0].content.slice(0, this.n);
  }

  async *_streamResponseChunks(
    messages: BaseMessage[],
    options: this["ParsedCallOptions"],
    runManager?: CallbackManagerForLLMRun
  ): AsyncGenerator<ChatGenerationChunk> {
    if (!messages.length) {
      throw new Error("未提供消息。");
    }
    if (typeof messages[0].content !== "string") {
      throw new Error("不支持多模态消息。");
    }
    // 在调用内部运行时，传递 `runManager?.getChild()` 以启用跟踪
    // await subRunnable.invoke(params, runManager?.getChild());
    for (const letter of messages[0].content.slice(0, this.n)) {
      yield new ChatGenerationChunk({
        message: new AIMessageChunk({
          content: letter,
        }),
        text: letter,
      });
      await runManager?.handleLLMNewToken(letter);
    }
  }
}
```

现在我们可以像使用其他聊天模型一样使用它：

```typescript
const chatModel = new CustomChatModel({ n: 4 });

await chatModel.invoke([["human", "我是一个LLM"]]);
```

```
AIMessage {
  content: '我是',
  additional_kwargs: {}
}
```

并支持流式处理：

```typescript
const stream = await chatModel.stream([["human", "我是一个LLM"]]);

for await (const chunk of stream) {
  console.log(chunk);
}
```

```
AIMessageChunk {
  content: '我',
  additional_kwargs: {}
}
AIMessageChunk {
  content: '是',
  additional_kwargs: {}
}
AIMessageChunk {
  content: '一',
  additional_kwargs: {}
}
AIMessageChunk {
  content: '个',
  additional_kwargs: {}
}
```

## 更丰富的输出

如果您想利用 LangChain 的回调系统来实现诸如令牌跟踪之类的功能，您可以扩展 [`BaseChatModel`](https://api.js.langchain.com/classes/langchain_core_language_models_chat_models.BaseChatModel.html) 类并实现更底层的 `_generate` 方法。它也接受 `BaseMessage` 列表作为输入，但需要您构建并返回一个允许附加元数据的 `ChatGeneration` 对象。
以下是一个示例：

```ts
import { AIMessage, BaseMessage } from "@langchain/core/messages";
import { ChatResult } from "@langchain/core/outputs";
import {
  BaseChatModel,
  BaseChatModelCallOptions,
  BaseChatModelParams,
} from "@langchain/core/language_models/chat_models";
import { CallbackManagerForLLMRun } from "@langchain/core/callbacks/manager";

export interface AdvancedCustomChatModelOptions
  extends BaseChatModelCallOptions {}

export interface AdvancedCustomChatModelParams extends BaseChatModelParams {
  n: number;
}

export class AdvancedCustomChatModel extends BaseChatModel<AdvancedCustomChatModelOptions> {
  n: number;

  static lc_name(): string {
    return "AdvancedCustomChatModel";
  }

  constructor(fields: AdvancedCustomChatModelParams) {
    super(fields);
    this.n = fields.n;
  }

  async _generate(
    messages: BaseMessage[],
    options: this["ParsedCallOptions"],
    runManager?: CallbackManagerForLLMRun
  ): Promise<ChatResult> {
    if (!messages.length) {
      throw new Error("未提供消息。");
    }
    if (typeof messages[0].content !== "string") {
      throw new Error("不支持多模态消息。");
    }
    // 在调用内部运行时可传递 `runManager?.getChild()` 以启用跟踪
    // await subRunnable.invoke(params, runManager?.getChild());
    const content = messages[0].content.slice(0, this.n);
    const tokenUsage = {
      usedTokens: this.n,
    };
    return {
      generations: [{ message: new AIMessage({ content }), text: content }],
      llmOutput: { tokenUsage },
    };
  }

  _llmType(): string {
    return "advanced_custom_chat_model";
  }
}
```

这将在回调事件和 `streamEvents` 方法中传递额外返回的信息：

```ts
const chatModel = new AdvancedCustomChatModel({ n: 4 });

const eventStream = await chatModel.streamEvents([["human", "I am an LLM"]], {
  version: "v1",
});

for await (const event of eventStream) {
  if (event.event === "on_llm_end") {
    console.log(JSON.stringify(event, null, 2));
  }
}
```

```
{
  "event": "on_llm_end",
  "name": "AdvancedCustomChatModel",
  "run_id": "b500b98d-bee5-4805-9b92-532a491f5c70",
  "tags": [],
  "metadata": {},
  "data": {
    "output": {
      "generations": [
        [
          {
            "message": {
              "lc": 1,
              "type": "constructor",
              "id": [
                "langchain_core",
                "messages",
                "AIMessage"
              ],
              "kwargs": {
                "content": "I am",
                "additional_kwargs": {}
              }
            },
            "text": "I am"
          }
        ]
      ],
      "llmOutput": {
        "tokenUsage": {
          "usedTokens": 4
        }
      }
    }
  }
}
```

